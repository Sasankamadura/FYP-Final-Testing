{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c62a1841",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 ‚Äî GPU Check & Install Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6185d7c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 ‚Äî GPU Check & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8979f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1a. Verify GPU\n",
    "!nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv\n",
    "!nvidia-smi | grep 'CUDA Version'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d13991a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1b. Install packages\n",
    "!pip install -q onnxruntime-gpu>=1.16.0 onnx pycocotools>=2.0.7 \\\n",
    "    PyYAML>=6.0 matplotlib>=3.7.0 pandas>=2.0.0 opencv-python-headless>=4.8.0\n",
    "\n",
    "import onnxruntime as ort\n",
    "print(f\"\\n\\u2705 ONNX Runtime {ort.__version__}\")\n",
    "print(f\"   Providers: {ort.get_available_providers()}\")\n",
    "assert 'CUDAExecutionProvider' in ort.get_available_providers(), \\\n",
    "    \"\\u274c CUDAExecutionProvider not found! Runtime > Change runtime type > GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74cb16e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 ‚Äî Clone Repository\n",
    "\n",
    "This pulls **everything** ‚Äî code, images, annotations, AND all ONNX models (via Git LFS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c5ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Clone repo with LFS models\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/Sasankamadura/FYP-Final-Testing.git\"\n",
    "WORKSPACE = \"/content/FYP-Final-Testing\"\n",
    "\n",
    "if os.path.exists(WORKSPACE):\n",
    "    print(f\"Repo already exists at {WORKSPACE}, pulling latest...\")\n",
    "    !cd \"{WORKSPACE}\" && git pull && git lfs pull\n",
    "else:\n",
    "    # Install Git LFS and clone\n",
    "    !git lfs install\n",
    "    !git clone \"{REPO_URL}\" \"{WORKSPACE}\"\n",
    "\n",
    "os.chdir(WORKSPACE)\n",
    "print(f\"\\n\\u2705 Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify LFS files were downloaded (not just pointers)\n",
    "!git lfs ls-files | head -5\n",
    "onnx_count = sum(1 for r, d, fs in os.walk('.') for f in fs if f.endswith('.onnx'))\n",
    "print(f\"\\n\\u2705 ONNX model files: {onnx_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89fb40e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 ‚Äî Verify Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1e71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3. Check everything is in place\n",
    "import os, yaml\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "print(\"\\u2501\" * 60)\n",
    "print(\"  DIRECTORY CHECK\")\n",
    "print(\"\\u2501\" * 60)\n",
    "for d in ['eval', 'eval/utils',\n",
    "          'Checkpoints - Baseline_Visdrone2019',\n",
    "          'Checkpoints - After improvements',\n",
    "          'VisDrone Val image set',\n",
    "          'VisDrone test image set/images']:\n",
    "    print(f\"  {'\\u2705' if os.path.isdir(d) else '\\u274c'} {d}\")\n",
    "\n",
    "ann = 'VisDrone Val image set/annotations_VisDrone_val.json'\n",
    "print(f\"\\n  Annotations: {'\\u2705' if os.path.exists(ann) else '\\u274c'}\")\n",
    "\n",
    "val_imgs = [f for f in os.listdir('VisDrone Val image set') if f.endswith(('.jpg','.png'))]\n",
    "test_imgs = [f for f in os.listdir('VisDrone test image set/images') if f.endswith(('.jpg','.png'))]\n",
    "print(f\"  Val images:  {len(val_imgs)}\")\n",
    "print(f\"  Test images: {len(test_imgs)}\")\n",
    "\n",
    "print(f\"\\n\\u2501\" * 60)\n",
    "print(\"  MODEL CHECKPOINT VERIFICATION\")\n",
    "print(\"\\u2501\" * 60)\n",
    "with open('eval/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "all_ok = True\n",
    "for key, mcfg in config['models'].items():\n",
    "    exists = os.path.exists(mcfg['path'])\n",
    "    size_str = f\"({os.path.getsize(mcfg['path'])/(1024*1024):.1f} MB)\" if exists else \"\"\n",
    "    if not exists: all_ok = False\n",
    "    print(f\"  {'\\u2705' if exists else '\\u274c MISSING'} {mcfg['name']:<40} {size_str}\")\n",
    "\n",
    "print()\n",
    "if all_ok:\n",
    "    print(\"\\u2705 All models found! Ready to run.\")\n",
    "else:\n",
    "    print(\"\\u274c Some models missing. Try: git lfs pull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c654de3",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 ‚Äî Run Validation (COCO mAP)\n",
    "\n",
    "Computes mAP@50, mAP@50:95, AP-small/medium/large, per-class AP for all models.\n",
    "\n",
    "Has **resume support** ‚Äî if Colab disconnects, re-run and it skips completed models.\n",
    "\n",
    "> ‚è± ~30‚Äì60 min for all models on T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aa711b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4a. Run validation ‚Äî ALL models\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d6f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4b. (Optional) Validate a SINGLE model first\n",
    "# Uncomment one line to quick-test:\n",
    "\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model baseline_rtdetr_r18\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model gnconv_p2_crr\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model efficientnet_b2_p2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590f6c4",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 ‚Äî Run Latency Benchmark\n",
    "\n",
    "Measures mean/median/P95/P99 latency, FPS, GPU memory, model size, parameter count.\n",
    "\n",
    "> ‚è± ~10‚Äì20 min for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ade45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5a. Benchmark ‚Äî ALL models\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/run_benchmark.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952be22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5b. (Optional) Benchmark with end-to-end timing\n",
    "# !python eval/run_benchmark.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --e2e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d883b",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 ‚Äî Generate Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c3453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6. Generate tables, CSVs, and plots\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/generate_report.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b4e41",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 ‚Äî View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bde8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7a. Accuracy Comparison\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "csv_path = 'eval/results/reports/accuracy_comparison.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    display(df.style.format({\n",
    "        'mAP_50': '{:.4f}', 'mAP_50_95': '{:.4f}',\n",
    "        'mAP_small': '{:.4f}', 'mAP_medium': '{:.4f}', 'mAP_large': '{:.4f}',\n",
    "        'delta_mAP_50': '{:+.4f}'\n",
    "    }).background_gradient(subset=['mAP_50'], cmap='Greens'))\n",
    "else:\n",
    "    print('Run Steps 4 + 6 first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af92404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7b. Speed vs Accuracy\n",
    "csv_path = 'eval/results/reports/speed_accuracy_comparison.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    display(df.style.format({\n",
    "        'mAP_50': '{:.4f}', 'mAP_50_95': '{:.4f}', 'mAP_small': '{:.4f}',\n",
    "        'latency_ms': '{:.2f}', 'fps': '{:.1f}', 'size_mb': '{:.1f}',\n",
    "        'efficiency_score': '{:.2f}'\n",
    "    }).background_gradient(subset=['efficiency_score'], cmap='YlGn'))\n",
    "else:\n",
    "    print('Run Steps 4 + 5 + 6 first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b66ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7c. Per-Class AP@50\n",
    "csv_path = 'eval/results/reports/per_class_ap50.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    ap_cols = [c for c in df.columns if c.startswith('AP50_')]\n",
    "    display(df.style.format({c: '{:.4f}' for c in ap_cols})\n",
    "            .background_gradient(subset=ap_cols, cmap='YlOrRd', vmin=0, vmax=0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7d. Display All Plots\n",
    "from IPython.display import Image as IPImage, display\n",
    "import glob\n",
    "\n",
    "plots_dir = 'eval/results/reports/plots'\n",
    "if os.path.isdir(plots_dir):\n",
    "    for pf in sorted(glob.glob(os.path.join(plots_dir, '*.png'))):\n",
    "        print(f\"\\n\\u2500\\u2500 {os.path.basename(pf)} \\u2500\\u2500\")\n",
    "        display(IPImage(filename=pf, width=900))\n",
    "else:\n",
    "    print('Run Step 6 first.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9606e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7e. Raw JSON results summary\n",
    "import json\n",
    "results_root = 'eval/results'\n",
    "for gpu_dir in sorted(d for d in os.listdir(results_root)\n",
    "                      if os.path.isdir(os.path.join(results_root, d)) and d.startswith('GPU_')):\n",
    "    val_file = os.path.join(results_root, gpu_dir, 'validation', 'all_validation_results.json')\n",
    "    if os.path.exists(val_file):\n",
    "        with open(val_file) as f: val = json.load(f)\n",
    "        print(f\"\\n\\u2501 {gpu_dir} \\u2014 {len(val)} models\")\n",
    "        for k, v in val.items():\n",
    "            m = v['metrics']\n",
    "            print(f\"  {v['name']:<40} mAP@50={m['mAP_50']:.4f}  AP-S={m['mAP_small']:.4f}\")\n",
    "\n",
    "    bench_file = os.path.join(results_root, gpu_dir, 'benchmark', 'benchmark_results.json')\n",
    "    if os.path.exists(bench_file):\n",
    "        with open(bench_file) as f: bench = json.load(f)\n",
    "        print(f\"\\n  Benchmark:\")\n",
    "        for k, v in bench.items():\n",
    "            print(f\"  {v['name']:<40} {v['mean_latency_ms']:.2f}ms  FPS={v['fps']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786b515",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 ‚Äî Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c193328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8a. Download as zip\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_path = '/content/eval_results'\n",
    "shutil.make_archive(zip_path, 'zip', os.path.join(WORKSPACE, 'eval/results'))\n",
    "size_mb = os.path.getsize(zip_path + '.zip') / (1024*1024)\n",
    "print(f\"\\u2705 {zip_path}.zip ({size_mb:.1f} MB)\")\n",
    "files.download(f'{zip_path}.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6afa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8b. (Optional) Push Colab results back to GitHub\n",
    "# os.chdir(WORKSPACE)\n",
    "# !git add eval/results/\n",
    "# !git commit -m \"Add Colab T4 GPU evaluation results\"\n",
    "# !git push origin main\n",
    "#\n",
    "# # For private repos use a Personal Access Token:\n",
    "# # !git push https://<YOUR_TOKEN>@github.com/Sasankamadura/FYP-Final-Testing.git main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487835b8",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Tips\n",
    "\n",
    "| Tip | Details |\n",
    "|-----|---------|\n",
    "| **Resume support** | If Colab disconnects, re-run Step 4 ‚Äî already-evaluated models are skipped |\n",
    "| **Single model test** | Use `--model baseline_rtdetr_r18` to test one model quickly |\n",
    "| **E2E benchmark** | Add `--e2e` to also measure preprocess + postprocess time |\n",
    "| **Cross-GPU comparison** | Your repo has RTX 4050 results; after Colab run, report auto-generates cross-GPU table |\n",
    "| **Colab Pro** | A100 gives ~3‚Äì5x faster benchmarks than T4 |\n",
    "\n",
    "| Issue | Fix |\n",
    "|-------|-----|\n",
    "| `CUDAExecutionProvider not available` | Runtime ‚Üí Change runtime type ‚Üí GPU |\n",
    "| ONNX files are tiny (~1KB) | LFS pointers not downloaded. Run `!git lfs pull` |\n",
    "| `Session crashed` | Re-run Step 4 (resume skips done models) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1a. Verify GPU is available\n",
    "!nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv\n",
    "print()\n",
    "!nvidia-smi | grep 'CUDA Version'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03042fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1b. Install required packages\n",
    "!pip install -q onnxruntime-gpu>=1.16.0 onnx pycocotools>=2.0.7 \\\n",
    "    PyYAML>=6.0 matplotlib>=3.7.0 pandas>=2.0.0 opencv-python-headless>=4.8.0\n",
    "\n",
    "# Verify GPU provider is available\n",
    "import onnxruntime as ort\n",
    "print(f\"\\n\\u2705 ONNX Runtime {ort.__version__}\")\n",
    "print(f\"   Providers: {ort.get_available_providers()}\")\n",
    "assert 'CUDAExecutionProvider' in ort.get_available_providers(), \\\n",
    "    \"\\u274c CUDAExecutionProvider not found! Go to Runtime > Change runtime type > GPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df728eee",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 ‚Äî Clone Your GitHub Repository\n",
    "\n",
    "This pulls all code, images, annotations, and previous results.\n",
    "\n",
    "The ONNX model files are `.gitignore`d (too large for GitHub), so we'll get those from Google Drive in Step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fe5d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Clone the repo\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/Sasankamadura/FYP-Final-Testing.git\"\n",
    "WORKSPACE = \"/content/FYP-Final-Testing\"\n",
    "\n",
    "if os.path.exists(WORKSPACE):\n",
    "    print(f\"Repo already cloned at {WORKSPACE}, pulling latest...\")\n",
    "    !cd \"{WORKSPACE}\" && git pull\n",
    "else:\n",
    "    !git clone \"{REPO_URL}\" \"{WORKSPACE}\"\n",
    "\n",
    "os.chdir(WORKSPACE)\n",
    "print(f\"\\n\\u2705 Working directory: {os.getcwd()}\")\n",
    "print(f\"   Contents: {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dac116",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 ‚Äî Get ONNX Checkpoints from Google Drive\n",
    "\n",
    "Your `.onnx` files are gitignored (~1.8 GB total). You need to upload them to Google Drive first.\n",
    "\n",
    "### üìÅ How to prepare your Drive\n",
    "\n",
    "On your PC, copy **only** the two checkpoint folders to Google Drive:\n",
    "```\n",
    "My Drive/\n",
    "  FYP-Checkpoints/\n",
    "    Checkpoints - Baseline_Visdrone2019/\n",
    "      RT-DETR Resnet 18/\n",
    "        base_rtdetr.onnx\n",
    "    Checkpoints - After improvements/\n",
    "      1-P2-P3 fusion/\n",
    "        model_p2p3_F.onnx\n",
    "      2-Query IMP/\n",
    "        model (1).onnx\n",
    "      ... (all other subfolders with .onnx files)\n",
    "```\n",
    "\n",
    "Then run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fc0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3a. Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3b. Copy ONNX checkpoints into the cloned repo\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# ===========================================================\n",
    "# EDIT THIS to match YOUR Google Drive folder path\n",
    "# ===========================================================\n",
    "DRIVE_CHECKPOINTS = \"/content/drive/MyDrive/FYP-Checkpoints\"\n",
    "\n",
    "# The two folders to copy\n",
    "folders_to_copy = [\n",
    "    \"Checkpoints - Baseline_Visdrone2019\",\n",
    "    \"Checkpoints - After improvements\",\n",
    "]\n",
    "\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "for folder in folders_to_copy:\n",
    "    src = os.path.join(DRIVE_CHECKPOINTS, folder)\n",
    "    dst = os.path.join(WORKSPACE, folder)\n",
    "\n",
    "    if not os.path.exists(src):\n",
    "        print(f\"\\u274c Source not found: {src}\")\n",
    "        print(f\"   Check your DRIVE_CHECKPOINTS path!\")\n",
    "        continue\n",
    "\n",
    "    if os.path.exists(dst):\n",
    "        print(f\"\\u2705 Already exists: {folder} (skipping copy)\")\n",
    "    else:\n",
    "        print(f\"\\u23f3 Copying {folder} ...\")\n",
    "        shutil.copytree(src, dst)\n",
    "        print(f\"\\u2705 Copied: {folder}\")\n",
    "\n",
    "# Verify ONNX files are in place\n",
    "onnx_count = 0\n",
    "for root, dirs, files in os.walk(WORKSPACE):\n",
    "    for f in files:\n",
    "        if f.endswith('.onnx'):\n",
    "            onnx_count += 1\n",
    "print(f\"\\n\\u2705 Total ONNX model files found: {onnx_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d08079",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 ‚Äî Verify Workspace Structure\n",
    "Make sure all checkpoints, images, and annotations are accessible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876375d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Full workspace verification\n",
    "import os, yaml\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "# --- Check key directories ---\n",
    "print(\"\\u2501\" * 60)\n",
    "print(\"  DIRECTORY CHECK\")\n",
    "print(\"\\u2501\" * 60)\n",
    "expected_dirs = [\n",
    "    'eval',\n",
    "    'eval/utils',\n",
    "    'Checkpoints - Baseline_Visdrone2019',\n",
    "    'Checkpoints - After improvements',\n",
    "    'VisDrone Val image set',\n",
    "    'VisDrone test image set/images',\n",
    "]\n",
    "for d in expected_dirs:\n",
    "    exists = os.path.isdir(d)\n",
    "    print(f\"  {'\\u2705' if exists else '\\u274c'} {d}\")\n",
    "\n",
    "# --- Check annotations ---\n",
    "ann = 'VisDrone Val image set/annotations_VisDrone_val.json'\n",
    "print(f\"\\n  Annotations: {'\\u2705' if os.path.exists(ann) else '\\u274c'} {ann}\")\n",
    "\n",
    "# --- Count images ---\n",
    "val_imgs = [f for f in os.listdir('VisDrone Val image set') if f.endswith(('.jpg','.png'))]\n",
    "test_imgs = [f for f in os.listdir('VisDrone test image set/images') if f.endswith(('.jpg','.png'))]\n",
    "print(f\"  Val images: {len(val_imgs)}\")\n",
    "print(f\"  Test images: {len(test_imgs)}\")\n",
    "\n",
    "# --- Load config & check each model ---\n",
    "print(f\"\\n\\u2501\" * 60)\n",
    "print(\"  MODEL CHECKPOINT VERIFICATION\")\n",
    "print(\"\\u2501\" * 60)\n",
    "with open('eval/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "all_ok = True\n",
    "for key, mcfg in config['models'].items():\n",
    "    exists = os.path.exists(mcfg['path'])\n",
    "    status = '\\u2705' if exists else '\\u274c MISSING'\n",
    "    size_str = \"\"\n",
    "    if exists:\n",
    "        size_mb = os.path.getsize(mcfg['path']) / (1024*1024)\n",
    "        size_str = f\"({size_mb:.1f} MB)\"\n",
    "    else:\n",
    "        all_ok = False\n",
    "    print(f\"  {status} {mcfg['name']:<40} {size_str}\")\n",
    "\n",
    "print()\n",
    "if all_ok:\n",
    "    print(\"\\u2705 All models found! Ready to run evaluation.\")\n",
    "else:\n",
    "    print(\"\\u274c Some models are missing. Check your Google Drive path in Step 3b.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a4416",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 ‚Äî Run Validation (COCO mAP)\n",
    "\n",
    "Runs inference on the VisDrone val set and computes:\n",
    "- **mAP@50**, **mAP@50:95**\n",
    "- **AP-small**, **AP-medium**, **AP-large**\n",
    "- **Per-class AP@50** for all 10 VisDrone classes\n",
    "\n",
    "Results are saved per-model with **resume support** ‚Äî if Colab disconnects, re-run and it skips already-evaluated models.\n",
    "\n",
    "> ‚è± ~30‚Äì60 min for all 21 models on T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c66f2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5a. Run validation for ALL models\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450729cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5b. (Optional) Run validation for a SINGLE model (quick test)\n",
    "# Uncomment one of these to test a single model first:\n",
    "\n",
    "# os.chdir(WORKSPACE)\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model baseline_rtdetr_r18\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model gnconv_p2_crr\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model efficientnet_b2_p2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0598be8e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 ‚Äî Run Latency Benchmark\n",
    "\n",
    "Measures per-model:\n",
    "- **Mean / Median / P95 / P99 latency** (ms)\n",
    "- **FPS** (frames per second)\n",
    "- **GPU memory** usage\n",
    "- **Model size** (MB) and **parameter count**\n",
    "\n",
    "> ‚è± ~10‚Äì20 min for all models (50 warmup + 200 measure iterations each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa566a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6a. Run benchmark for ALL models\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/run_benchmark.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9b3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6b. (Optional) Also run end-to-end benchmark (includes preprocessing)\n",
    "# os.chdir(WORKSPACE)\n",
    "# !python eval/run_benchmark.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --e2e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbae7bec",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 ‚Äî Generate Report\n",
    "\n",
    "Aggregates validation + benchmark results into:\n",
    "- **Accuracy comparison table** (CSV)\n",
    "- **Per-class AP@50 table** (CSV)\n",
    "- **Speed vs Accuracy comparison** (CSV)\n",
    "- **3-dec vs 6-dec comparison**\n",
    "- **Plots**: mAP bar chart, Pareto scatter, AP by object size, per-class heatmap, FPS chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Generate full report\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/generate_report.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accbe7de",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 ‚Äî View Results Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74400178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8a. Accuracy Comparison Table\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "csv_path = 'eval/results/reports/accuracy_comparison.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\u2501\" * 80)\n",
    "    print(\"  ACCURACY COMPARISON\")\n",
    "    print(f\"\\u2501\" * 80)\n",
    "    display(df.style.format({\n",
    "        'mAP_50': '{:.4f}', 'mAP_50_95': '{:.4f}',\n",
    "        'mAP_small': '{:.4f}', 'mAP_medium': '{:.4f}', 'mAP_large': '{:.4f}',\n",
    "        'delta_mAP_50': '{:+.4f}'\n",
    "    }).background_gradient(subset=['mAP_50'], cmap='Greens'))\n",
    "else:\n",
    "    print(\"\\u274c Not found. Run Steps 5 + 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82511351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8b. Speed vs Accuracy Table\n",
    "csv_path = 'eval/results/reports/speed_accuracy_comparison.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\u2501\" * 80)\n",
    "    print(\"  SPEED vs ACCURACY\")\n",
    "    print(f\"\\u2501\" * 80)\n",
    "    display(df.style.format({\n",
    "        'mAP_50': '{:.4f}', 'mAP_50_95': '{:.4f}', 'mAP_small': '{:.4f}',\n",
    "        'latency_ms': '{:.2f}', 'fps': '{:.1f}', 'size_mb': '{:.1f}',\n",
    "        'efficiency_score': '{:.2f}'\n",
    "    }).background_gradient(subset=['efficiency_score'], cmap='YlGn'))\n",
    "else:\n",
    "    print(\"\\u274c Not found. Run Steps 5 + 6 + 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5318541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8c. Per-Class AP@50\n",
    "csv_path = 'eval/results/reports/per_class_ap50.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"PER-CLASS AP@50\")\n",
    "    # Color each AP column\n",
    "    ap_cols = [c for c in df.columns if c.startswith('AP50_')]\n",
    "    display(df.style.format({c: '{:.4f}' for c in ap_cols})\n",
    "            .background_gradient(subset=ap_cols, cmap='YlOrRd', vmin=0, vmax=0.7))\n",
    "else:\n",
    "    print(\"\\u274c Not found. Run Steps 5 + 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8d. Display All Generated Plots\n",
    "from IPython.display import Image as IPImage, display\n",
    "import glob\n",
    "\n",
    "plots_dir = 'eval/results/reports/plots'\n",
    "if os.path.isdir(plots_dir):\n",
    "    plot_files = sorted(glob.glob(os.path.join(plots_dir, '*.png')))\n",
    "    print(f\"\\u2705 Found {len(plot_files)} plots\\n\")\n",
    "    for pf in plot_files:\n",
    "        print(f\"\\u2500\\u2500 {os.path.basename(pf)} \\u2500\\u2500\")\n",
    "        display(IPImage(filename=pf, width=900))\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\u274c No plots found. Run Step 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcd7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8e. Quick look at raw JSON results\n",
    "import json\n",
    "\n",
    "# Find the GPU results folder for this Colab session\n",
    "results_root = 'eval/results'\n",
    "gpu_dirs = [d for d in os.listdir(results_root)\n",
    "            if os.path.isdir(os.path.join(results_root, d)) and d.startswith('GPU_')]\n",
    "print(f\"GPU result folders: {gpu_dirs}\\n\")\n",
    "\n",
    "for gpu_dir in gpu_dirs:\n",
    "    # Validation summary\n",
    "    val_file = os.path.join(results_root, gpu_dir, 'validation', 'all_validation_results.json')\n",
    "    if os.path.exists(val_file):\n",
    "        with open(val_file) as f:\n",
    "            val = json.load(f)\n",
    "        print(f\"\\u2501 {gpu_dir} \\u2014 Validation: {len(val)} models evaluated\")\n",
    "        for k, v in val.items():\n",
    "            m = v['metrics']\n",
    "            print(f\"  {v['name']:<40} mAP@50={m['mAP_50']:.4f}  mAP@50:95={m['mAP_50_95']:.4f}  AP-S={m['mAP_small']:.4f}\")\n",
    "\n",
    "    # Benchmark summary\n",
    "    bench_file = os.path.join(results_root, gpu_dir, 'benchmark', 'benchmark_results.json')\n",
    "    if os.path.exists(bench_file):\n",
    "        with open(bench_file) as f:\n",
    "            bench = json.load(f)\n",
    "        print(f\"\\n\\u2501 {gpu_dir} \\u2014 Benchmark: {len(bench)} models benchmarked\")\n",
    "        for k, v in bench.items():\n",
    "            print(f\"  {v['name']:<40} {v['mean_latency_ms']:.2f}ms  FPS={v['fps']:.1f}  P95={v['p95_latency_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805e80d2",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9 ‚Äî Save Results Back & Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54516d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9a. Copy results back to Google Drive (persistent storage)\n",
    "import shutil\n",
    "\n",
    "# Save to Drive so results survive Colab session resets\n",
    "DRIVE_RESULTS_DST = \"/content/drive/MyDrive/FYP-Checkpoints/Colab_Results\"\n",
    "src = os.path.join(WORKSPACE, 'eval/results')\n",
    "\n",
    "if os.path.exists(src):\n",
    "    if os.path.exists(DRIVE_RESULTS_DST):\n",
    "        shutil.rmtree(DRIVE_RESULTS_DST)\n",
    "    shutil.copytree(src, DRIVE_RESULTS_DST)\n",
    "    print(f\"\\u2705 Results copied to Google Drive: {DRIVE_RESULTS_DST}\")\n",
    "else:\n",
    "    print(\"\\u274c No results to copy yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08332e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9b. Download results as zip to your PC\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_path = '/content/eval_results'\n",
    "results_src = os.path.join(WORKSPACE, 'eval/results')\n",
    "\n",
    "if os.path.exists(results_src):\n",
    "    shutil.make_archive(zip_path, 'zip', results_src)\n",
    "    size_mb = os.path.getsize(zip_path + '.zip') / (1024 * 1024)\n",
    "    print(f\"\\u2705 Zipped: {zip_path}.zip ({size_mb:.1f} MB)\")\n",
    "    files.download(f'{zip_path}.zip')\n",
    "else:\n",
    "    print(\"\\u274c No results to download yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aae84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9c. (Optional) Git commit & push new Colab results back to repo\n",
    "# This pushes the Colab GPU results alongside your existing RTX 4050 results.\n",
    "# You'll need a GitHub Personal Access Token for private repos.\n",
    "\n",
    "# os.chdir(WORKSPACE)\n",
    "# !git config user.email \"your-email@example.com\"\n",
    "# !git config user.name \"Your Name\"\n",
    "# !git add eval/results/\n",
    "# !git commit -m \"Add Colab T4 GPU evaluation results\"\n",
    "# !git push origin main\n",
    "# # For private repos, use: git push https://<TOKEN>@github.com/Sasankamadura/FYP-Final-Testing.git main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f692301",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Tips & Troubleshooting\n",
    "\n",
    "### Resume Support\n",
    "The validation script **automatically skips** models that have already been evaluated.\n",
    "If your Colab session dies mid-run, just re-run Step 5 ‚Äî it picks up where it left off.\n",
    "\n",
    "### Cross-GPU Comparison\n",
    "Your repo already has results from `GPU_NVIDIA_GeForce_RTX_4050_Laptop_GPU`.\n",
    "After running on Colab (T4), the report generator will automatically produce a\n",
    "**cross-GPU FPS comparison** table.\n",
    "\n",
    "### Performance Tips\n",
    "| Tip | Details |\n",
    "|-----|---------|\n",
    "| **Faster I/O** | Checkpoints are already copied to local `/content/` disk (not read from Drive during eval) |\n",
    "| **Single model test** | Use `--model baseline_rtdetr_r18` flag to test one model quickly |\n",
    "| **E2E benchmark** | Add `--e2e` flag to also measure full pipeline (preprocess + inference + postprocess) |\n",
    "| **Colab Pro** | A100 GPU gives ~3‚Äì5x faster benchmarks than T4 |\n",
    "\n",
    "### Common Issues\n",
    "| Issue | Fix |\n",
    "|-------|-----|\n",
    "| `CUDAExecutionProvider not available` | Runtime ‚Üí Change runtime type ‚Üí GPU |\n",
    "| `FileNotFoundError` on `.onnx` file | Check Step 3b ‚Äî `DRIVE_CHECKPOINTS` path must match your Drive folder |\n",
    "| `Session crashed` during validation | Re-run Step 5 (resume support will skip completed models) |\n",
    "| `git clone` fails (private repo) | Use `!git clone https://<TOKEN>@github.com/Sasankamadura/FYP-Final-Testing.git` |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
