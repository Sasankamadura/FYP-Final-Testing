{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98c7b0ed",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1 ‚Äî GPU Check & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81baae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1a. Verify GPU is available\n",
    "!nvidia-smi --query-gpu=name,memory.total,driver_version --format=csv\n",
    "print()\n",
    "!nvidia-smi | grep 'CUDA Version'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1b. Install required packages\n",
    "!pip install -q onnxruntime-gpu>=1.16.0 onnx pycocotools>=2.0.7 \\\n",
    "    PyYAML>=6.0 matplotlib>=3.7.0 pandas>=2.0.0 opencv-python-headless>=4.8.0\n",
    "\n",
    "# Verify GPU provider is available\n",
    "import onnxruntime as ort\n",
    "print(f\"\\n‚úÖ ONNX Runtime {ort.__version__}\")\n",
    "print(f\"   Providers: {ort.get_available_providers()}\")\n",
    "assert 'CUDAExecutionProvider' in ort.get_available_providers(), \\\n",
    "    \"‚ùå CUDAExecutionProvider not found! Make sure you enabled GPU: Settings ‚Üí Accelerator ‚Üí GPU T4x2 or P100\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c2876",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2 ‚Äî Clone Your GitHub Repository\n",
    "\n",
    "This pulls the evaluation code, utility scripts, and config. ONNX models and images come from Kaggle datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4969f029",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2. Clone the repo\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/Sasankamadura/FYP-Final-Testing.git\"\n",
    "WORKSPACE = \"/kaggle/working/FYP-Final-Testing\"\n",
    "\n",
    "if os.path.exists(WORKSPACE):\n",
    "    print(f\"Repo already cloned at {WORKSPACE}, pulling latest...\")\n",
    "    !cd \"{WORKSPACE}\" && git pull\n",
    "else:\n",
    "    !git clone \"{REPO_URL}\" \"{WORKSPACE}\"\n",
    "\n",
    "os.chdir(WORKSPACE)\n",
    "print(f\"\\n‚úÖ Working directory: {os.getcwd()}\")\n",
    "print(f\"   Contents: {os.listdir('.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bea6e",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3 ‚Äî Link Kaggle Datasets into Workspace\n",
    "\n",
    "Kaggle datasets are mounted **read-only** under `/kaggle/input/`.\n",
    "We create **symlinks** so the eval scripts find files at the expected paths.\n",
    "\n",
    "### ‚ö†Ô∏è Update the dataset slugs below to match YOUR uploaded dataset names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0b3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3a. Configure Kaggle dataset paths\n",
    "# ============================================================\n",
    "# UPDATE THESE to match your Kaggle dataset slugs!\n",
    "# Go to the \"Input\" panel on the right to see the exact paths.\n",
    "# ============================================================\n",
    "\n",
    "# Your Kaggle username (used in dataset paths)\n",
    "KAGGLE_USER = \"your-kaggle-username\"  # <-- CHANGE THIS\n",
    "\n",
    "# Dataset 1: ONNX checkpoints\n",
    "CHECKPOINTS_DATASET = f\"/kaggle/input/fyp-visdrone-checkpoints\"\n",
    "\n",
    "# Dataset 2: VisDrone images + annotations\n",
    "VISDRONE_DATASET = f\"/kaggle/input/fyp-visdrone-dataset\"\n",
    "\n",
    "print(\"Kaggle input datasets:\")\n",
    "print(f\"  Checkpoints: {CHECKPOINTS_DATASET}\")\n",
    "print(f\"  VisDrone:    {VISDRONE_DATASET}\")\n",
    "\n",
    "# Quick sanity check\n",
    "for p in [CHECKPOINTS_DATASET, VISDRONE_DATASET]:\n",
    "    if os.path.exists(p):\n",
    "        print(f\"  ‚úÖ Found: {p}\")\n",
    "        print(f\"     Contents: {os.listdir(p)[:10]}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå NOT FOUND: {p}\")\n",
    "        print(f\"     ‚Üí Check 'Add Data' panel on the right. Available inputs:\")\n",
    "        if os.path.exists('/kaggle/input'):\n",
    "            print(f\"       {os.listdir('/kaggle/input')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7349d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3b. Create symlinks into workspace\n",
    "import os\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "# ---- Symlink checkpoint folders ----\n",
    "links = {\n",
    "    \"Checkpoints - Baseline_Visdrone2019\": os.path.join(CHECKPOINTS_DATASET, \"Checkpoints - Baseline_Visdrone2019\"),\n",
    "    \"Checkpoints - After improvements\":    os.path.join(CHECKPOINTS_DATASET, \"Checkpoints - After improvements\"),\n",
    "    \"VisDrone Val image set\":               os.path.join(VISDRONE_DATASET, \"VisDrone Val image set\"),\n",
    "    \"VisDrone test image set\":              os.path.join(VISDRONE_DATASET, \"VisDrone test image set\"),\n",
    "}\n",
    "\n",
    "for link_name, target in links.items():\n",
    "    link_path = os.path.join(WORKSPACE, link_name)\n",
    "    \n",
    "    # Remove existing link/dir if present\n",
    "    if os.path.islink(link_path):\n",
    "        os.unlink(link_path)\n",
    "    elif os.path.isdir(link_path):\n",
    "        import shutil\n",
    "        shutil.rmtree(link_path)\n",
    "    \n",
    "    if os.path.exists(target):\n",
    "        os.symlink(target, link_path)\n",
    "        print(f\"‚úÖ {link_name} ‚Üí {target}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Target not found: {target}\")\n",
    "        print(f\"   Available in dataset root: {os.listdir(os.path.dirname(target))}\")\n",
    "\n",
    "print(\"\\nüìÅ Workspace contents:\")\n",
    "for item in sorted(os.listdir(WORKSPACE)):\n",
    "    full = os.path.join(WORKSPACE, item)\n",
    "    suffix = \" ‚Üí \" + os.readlink(full) if os.path.islink(full) else \"\"\n",
    "    print(f\"  {'üìÅ' if os.path.isdir(full) else 'üìÑ'} {item}{suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c023501f",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4 ‚Äî Verify Workspace\n",
    "\n",
    "Checks all directories, annotations, images, and model checkpoint files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb244c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4. Full workspace verification\n",
    "import os, yaml\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "# --- Check key directories ---\n",
    "print(\"‚îÅ\" * 60)\n",
    "print(\"  DIRECTORY CHECK\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "expected_dirs = [\n",
    "    'eval',\n",
    "    'eval/utils',\n",
    "    'Checkpoints - Baseline_Visdrone2019',\n",
    "    'Checkpoints - After improvements',\n",
    "    'VisDrone Val image set',\n",
    "    'VisDrone test image set/images',\n",
    "]\n",
    "for d in expected_dirs:\n",
    "    exists = os.path.isdir(d)\n",
    "    print(f\"  {'‚úÖ' if exists else '‚ùå'} {d}\")\n",
    "\n",
    "# --- Check annotations ---\n",
    "ann = 'VisDrone Val image set/annotations_VisDrone_val.json'\n",
    "print(f\"\\n  Annotations: {'‚úÖ' if os.path.exists(ann) else '‚ùå'} {ann}\")\n",
    "\n",
    "# --- Count images ---\n",
    "val_imgs = [f for f in os.listdir('VisDrone Val image set') if f.endswith(('.jpg','.png'))]\n",
    "test_imgs = [f for f in os.listdir('VisDrone test image set/images') if f.endswith(('.jpg','.png'))]\n",
    "print(f\"  Val images: {len(val_imgs)}\")\n",
    "print(f\"  Test images: {len(test_imgs)}\")\n",
    "\n",
    "# --- Load config & check each model ---\n",
    "print(f\"\\n‚îÅ\" * 60)\n",
    "print(\"  MODEL CHECKPOINT VERIFICATION\")\n",
    "print(\"‚îÅ\" * 60)\n",
    "with open('eval/config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "all_ok = True\n",
    "for key, mcfg in config['models'].items():\n",
    "    exists = os.path.exists(mcfg['path'])\n",
    "    status = '‚úÖ' if exists else '‚ùå MISSING'\n",
    "    size_str = \"\"\n",
    "    if exists:\n",
    "        size_mb = os.path.getsize(mcfg['path']) / (1024*1024)\n",
    "        size_str = f\"({size_mb:.1f} MB)\"\n",
    "    else:\n",
    "        all_ok = False\n",
    "    print(f\"  {status} {mcfg['name']:<40} {size_str}\")\n",
    "\n",
    "print()\n",
    "if all_ok:\n",
    "    print(\"‚úÖ All models found! Ready to run evaluation.\")\n",
    "else:\n",
    "    print(\"‚ùå Some models are missing. Check your Kaggle dataset paths in Step 3a.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282293a0",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4b ‚Äî Visual Sanity Check (Predictions vs Ground Truth)\n",
    "\n",
    "Runs the **baseline model** on a random validation image and displays:\n",
    "- **Left**: Ground-truth annotations (from COCO JSON)\n",
    "- **Right**: Model predictions (from ONNX inference)\n",
    "\n",
    "This verifies the **category mapping is correct** ‚Äî class labels on both sides should match (e.g. a car should be labelled \"car\", not \"van\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32bf9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4b. Visual comparison ‚Äî Ground Truth vs Baseline Predictions\n",
    "import os, sys, json, random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import yaml\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "os.chdir(WORKSPACE)\n",
    "sys.path.insert(0, os.path.join(WORKSPACE, \"eval\"))\n",
    "\n",
    "from utils.onnx_inference import OnnxDetector\n",
    "from utils.visualization import COLORS, CLASS_NAMES\n",
    "\n",
    "# ‚îÄ‚îÄ Load config & annotations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "with open(\"eval/config.yaml\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "ann_path = os.path.join(WORKSPACE, config[\"datasets\"][\"val\"][\"annotations\"])\n",
    "with open(ann_path) as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "cat_map = {c[\"id\"]: c[\"name\"] for c in coco_data[\"categories\"]}\n",
    "images_dir = os.path.join(WORKSPACE, config[\"datasets\"][\"val\"][\"images_dir\"])\n",
    "\n",
    "# ‚îÄ‚îÄ Pick a random image that has enough annotations ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "from collections import Counter\n",
    "ann_per_img = Counter(a[\"image_id\"] for a in coco_data[\"annotations\"])\n",
    "rich_images = [img for img in coco_data[\"images\"] if ann_per_img.get(img[\"id\"], 0) >= 10]\n",
    "img_info = random.choice(rich_images)\n",
    "img_path = os.path.join(images_dir, img_info[\"file_name\"])\n",
    "image = cv2.imread(img_path)\n",
    "orig_h, orig_w = image.shape[:2]\n",
    "print(f\"Selected image: {img_info['file_name']} ({orig_w}x{orig_h}, \"\n",
    "      f\"{ann_per_img[img_info['id']]} GT annotations)\")\n",
    "\n",
    "# ‚îÄ‚îÄ Draw Ground-Truth ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "gt_img = image.copy()\n",
    "gt_anns = [a for a in coco_data[\"annotations\"] if a[\"image_id\"] == img_info[\"id\"]]\n",
    "for ann in gt_anns:\n",
    "    x, y, w, h = ann[\"bbox\"]\n",
    "    cid = ann[\"category_id\"]\n",
    "    idx = cid - 1\n",
    "    color = COLORS[idx % len(COLORS)]\n",
    "    name = cat_map.get(cid, f\"cls{cid}\")\n",
    "    cv2.rectangle(gt_img, (int(x), int(y)), (int(x+w), int(y+h)), color, 2)\n",
    "    label = f\"GT: {name}\"\n",
    "    (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)\n",
    "    cv2.rectangle(gt_img, (int(x), int(y)-th-6), (int(x)+tw, int(y)), color, -1)\n",
    "    cv2.putText(gt_img, label, (int(x), int(y)-4),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 1)\n",
    "\n",
    "# ‚îÄ‚îÄ Run Baseline Model Inference ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "baseline_cfg = config[\"models\"][\"baseline_rtdetr_r18\"]\n",
    "model_path = os.path.join(WORKSPACE, baseline_cfg[\"path\"])\n",
    "detector = OnnxDetector(model_path, input_size=(640, 640))\n",
    "\n",
    "boxes, scores, class_ids = detector.predict(image, conf_threshold=0.3)\n",
    "print(f\"Baseline predictions (conf>0.3): {len(scores)} detections\")\n",
    "print(f\"  Unique model labels: {sorted(set(int(c) for c in class_ids))}\")\n",
    "\n",
    "pred_img = image.copy()\n",
    "for i in range(len(boxes)):\n",
    "    x1, y1, x2, y2 = boxes[i].astype(int)\n",
    "    cid = int(class_ids[i])\n",
    "    idx = cid - 1\n",
    "    color = COLORS[idx % len(COLORS)]\n",
    "    name = cat_map.get(cid, f\"cls{cid}\")\n",
    "    cv2.rectangle(pred_img, (x1, y1), (x2, y2), color, 2)\n",
    "    label = f\"{name} {scores[i]:.2f}\"\n",
    "    (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)\n",
    "    cv2.rectangle(pred_img, (x1, y1-th-6), (x1+tw, y1), color, -1)\n",
    "    cv2.putText(pred_img, label, (x1, y1-4),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 1)\n",
    "\n",
    "# ‚îÄ‚îÄ Combine side-by-side ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "target_w = 700\n",
    "scale = target_w / orig_w\n",
    "target_h = int(orig_h * scale)\n",
    "gt_resized = cv2.resize(gt_img, (target_w, target_h))\n",
    "pred_resized = cv2.resize(pred_img, (target_w, target_h))\n",
    "\n",
    "# Add titles\n",
    "for panel, title in [(gt_resized, \"GROUND TRUTH\"), (pred_resized, \"BASELINE PREDICTIONS\")]:\n",
    "    cv2.putText(panel, title, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,0), 4)\n",
    "    cv2.putText(panel, title, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,255,255), 2)\n",
    "\n",
    "divider = np.full((target_h, 4, 3), 200, dtype=np.uint8)\n",
    "comparison = np.hstack([gt_resized, divider, pred_resized])\n",
    "\n",
    "# ‚îÄ‚îÄ Build legend ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "used_ids = sorted(set(a[\"category_id\"] for a in gt_anns) | set(int(c) for c in class_ids))\n",
    "legend_h = 35\n",
    "legend = np.full((legend_h, comparison.shape[1], 3), 40, dtype=np.uint8)\n",
    "x_off = 10\n",
    "for cid in used_ids:\n",
    "    idx = cid - 1\n",
    "    color = COLORS[idx % len(COLORS)]\n",
    "    name = cat_map.get(cid, f\"cls{cid}\")\n",
    "    cv2.rectangle(legend, (x_off, 8), (x_off+18, 26), color, -1)\n",
    "    cv2.putText(legend, name, (x_off+22, 23), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,255), 1)\n",
    "    x_off += 22 + len(name)*9 + 15\n",
    "\n",
    "final = np.vstack([comparison, legend])\n",
    "\n",
    "# ‚îÄ‚îÄ Display ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "out_path = \"/tmp/sanity_check_comparison.png\"\n",
    "cv2.imwrite(out_path, final)\n",
    "display(IPImage(filename=out_path, width=1404))\n",
    "\n",
    "# ‚îÄ‚îÄ Category mapping verification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  CATEGORY MAPPING VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "for cid in sorted(cat_map.keys()):\n",
    "    print(f\"  model label {cid} -> cat_id {cid} = {cat_map[cid]}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"If a car in GT is labelled 'car' in predictions (not 'van'),\")\n",
    "print(\"the category mapping is correct. Safe to run full validation!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35764198",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5 ‚Äî Run Validation (COCO mAP)\n",
    "\n",
    "Runs inference on the VisDrone val set and computes:\n",
    "- **mAP@50**, **mAP@50:95**\n",
    "- **AP-small**, **AP-medium**, **AP-large**\n",
    "- **Per-class AP@50** for all 10 VisDrone classes\n",
    "\n",
    "Results are saved per-model with **resume support** ‚Äî if Kaggle session restarts, re-run and it skips already-evaluated models.\n",
    "\n",
    "> ‚è± ~30‚Äì60 min for all 21 models on T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a93b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5a. Run validation for ALL models\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5b. (Optional) Run validation for a SINGLE model (quick test)\n",
    "# Uncomment one of these to test a single model first:\n",
    "\n",
    "# os.chdir(WORKSPACE)\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model baseline_rtdetr_r18\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model gnconv_p2_crr\n",
    "# !python eval/run_validation.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --model efficientnet_b2_p2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a257cc55",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6 ‚Äî Run Latency Benchmark\n",
    "\n",
    "Measures per-model:\n",
    "- **Mean / Median / P95 / P99 latency** (ms)\n",
    "- **FPS** (frames per second)\n",
    "- **GPU memory** usage\n",
    "- **Model size** (MB) and **parameter count**\n",
    "\n",
    "> ‚è± ~10‚Äì20 min for all models (50 warmup + 200 measure iterations each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6a. Run benchmark for ALL models\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/run_benchmark.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc1e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6b. (Optional) Also run end-to-end benchmark (includes preprocessing)\n",
    "# os.chdir(WORKSPACE)\n",
    "# !python eval/run_benchmark.py --config eval/config.yaml --workspace \"{WORKSPACE}\" --e2e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a649f86",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7 ‚Äî Generate Report\n",
    "\n",
    "Aggregates validation + benchmark results into:\n",
    "- **Accuracy comparison table** (CSV)\n",
    "- **Per-class AP@50 table** (CSV)\n",
    "- **Speed vs Accuracy comparison** (CSV)\n",
    "- **3-dec vs 6-dec comparison**\n",
    "- **Plots**: mAP bar chart, Pareto scatter, AP by object size, per-class heatmap, FPS chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e5407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 7. Generate full report\n",
    "os.chdir(WORKSPACE)\n",
    "!python eval/generate_report.py --config eval/config.yaml --workspace \"{WORKSPACE}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d40ab",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8 ‚Äî View Results Inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc11623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8a. Accuracy Comparison Table\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "csv_path = 'eval/results/reports/accuracy_comparison.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚îÅ\" * 80)\n",
    "    print(\"  ACCURACY COMPARISON\")\n",
    "    print(f\"‚îÅ\" * 80)\n",
    "    display(df.style.format({\n",
    "        'mAP_50': '{:.4f}', 'mAP_50_95': '{:.4f}',\n",
    "        'mAP_small': '{:.4f}', 'mAP_medium': '{:.4f}', 'mAP_large': '{:.4f}',\n",
    "        'delta_mAP_50': '{:+.4f}'\n",
    "    }).background_gradient(subset=['mAP_50'], cmap='Greens'))\n",
    "else:\n",
    "    print(\"‚ùå Not found. Run Steps 5 + 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fee7bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8b. Speed vs Accuracy Table\n",
    "csv_path = 'eval/results/reports/speed_accuracy_comparison.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"‚îÅ\" * 80)\n",
    "    print(\"  SPEED vs ACCURACY\")\n",
    "    print(f\"‚îÅ\" * 80)\n",
    "    display(df.style.format({\n",
    "        'mAP_50': '{:.4f}', 'mAP_50_95': '{:.4f}', 'mAP_small': '{:.4f}',\n",
    "        'latency_ms': '{:.2f}', 'fps': '{:.1f}', 'size_mb': '{:.1f}',\n",
    "        'efficiency_score': '{:.2f}'\n",
    "    }).background_gradient(subset=['efficiency_score'], cmap='YlGn'))\n",
    "else:\n",
    "    print(\"‚ùå Not found. Run Steps 5 + 6 + 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37764e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8c. Per-Class AP@50\n",
    "csv_path = 'eval/results/reports/per_class_ap50.csv'\n",
    "if os.path.exists(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"PER-CLASS AP@50\")\n",
    "    # Color each AP column\n",
    "    ap_cols = [c for c in df.columns if c.startswith('AP50_')]\n",
    "    display(df.style.format({c: '{:.4f}' for c in ap_cols})\n",
    "            .background_gradient(subset=ap_cols, cmap='YlOrRd', vmin=0, vmax=0.7))\n",
    "else:\n",
    "    print(\"‚ùå Not found. Run Steps 5 + 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb619677",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8d. Display All Generated Plots\n",
    "from IPython.display import Image as IPImage, display\n",
    "import glob\n",
    "\n",
    "plots_dir = 'eval/results/reports/plots'\n",
    "if os.path.isdir(plots_dir):\n",
    "    plot_files = sorted(glob.glob(os.path.join(plots_dir, '*.png')))\n",
    "    print(f\"‚úÖ Found {len(plot_files)} plots\\n\")\n",
    "    for pf in plot_files:\n",
    "        print(f\"‚îÄ‚îÄ {os.path.basename(pf)} ‚îÄ‚îÄ\")\n",
    "        display(IPImage(filename=pf, width=900))\n",
    "        print()\n",
    "else:\n",
    "    print(\"‚ùå No plots found. Run Step 7 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9e64b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 8e. Quick look at raw JSON results\n",
    "import json\n",
    "\n",
    "# Find the GPU results folder for this Kaggle session\n",
    "results_root = 'eval/results'\n",
    "gpu_dirs = [d for d in os.listdir(results_root)\n",
    "            if os.path.isdir(os.path.join(results_root, d)) and d.startswith('GPU_')]\n",
    "print(f\"GPU result folders: {gpu_dirs}\\n\")\n",
    "\n",
    "for gpu_dir in gpu_dirs:\n",
    "    # Validation summary\n",
    "    val_file = os.path.join(results_root, gpu_dir, 'validation', 'all_validation_results.json')\n",
    "    if os.path.exists(val_file):\n",
    "        with open(val_file) as f:\n",
    "            val = json.load(f)\n",
    "        print(f\"‚îÅ {gpu_dir} ‚Äî Validation: {len(val)} models evaluated\")\n",
    "        for k, v in val.items():\n",
    "            m = v['metrics']\n",
    "            print(f\"  {v['name']:<40} mAP@50={m['mAP_50']:.4f}  mAP@50:95={m['mAP_50_95']:.4f}  AP-S={m['mAP_small']:.4f}\")\n",
    "\n",
    "    # Benchmark summary\n",
    "    bench_file = os.path.join(results_root, gpu_dir, 'benchmark', 'benchmark_results.json')\n",
    "    if os.path.exists(bench_file):\n",
    "        with open(bench_file) as f:\n",
    "            bench = json.load(f)\n",
    "        print(f\"\\n‚îÅ {gpu_dir} ‚Äî Benchmark: {len(bench)} models benchmarked\")\n",
    "        for k, v in bench.items():\n",
    "            print(f\"  {v['name']:<40} {v['mean_latency_ms']:.2f}ms  FPS={v['fps']:.1f}  P95={v['p95_latency_ms']:.2f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2719bf1",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9 ‚Äî Save & Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ed426",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9a. Zip results for download\n",
    "import shutil\n",
    "os.chdir(WORKSPACE)\n",
    "\n",
    "results_src = os.path.join(WORKSPACE, 'eval/results')\n",
    "zip_path = '/kaggle/working/eval_results'\n",
    "\n",
    "if os.path.exists(results_src):\n",
    "    shutil.make_archive(zip_path, 'zip', results_src)\n",
    "    size_mb = os.path.getsize(zip_path + '.zip') / (1024 * 1024)\n",
    "    print(f\"‚úÖ Zipped: {zip_path}.zip ({size_mb:.1f} MB)\")\n",
    "    print(f\"\\nüì• Download from the Output tab on the right panel ‚Üí\")\n",
    "    print(f\"   Or find it at: /kaggle/working/eval_results.zip\")\n",
    "else:\n",
    "    print(\"‚ùå No results to download yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9b. Save results as Kaggle output dataset\n",
    "# Everything in /kaggle/working/ is automatically saved as notebook output.\n",
    "# Copy results to the working directory root so they appear in Output.\n",
    "import shutil\n",
    "\n",
    "output_dst = '/kaggle/working/eval_results_output'\n",
    "results_src = os.path.join(WORKSPACE, 'eval/results')\n",
    "\n",
    "if os.path.exists(results_src):\n",
    "    if os.path.exists(output_dst):\n",
    "        shutil.rmtree(output_dst)\n",
    "    shutil.copytree(results_src, output_dst)\n",
    "    print(f\"‚úÖ Results copied to {output_dst}\")\n",
    "    print(\"   These will be available in the notebook Output after 'Save & Run All'.\")\n",
    "else:\n",
    "    print(\"‚ùå No results to copy yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec7343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 9c. (Optional) Git commit & push new Kaggle results back to repo\n",
    "# This pushes the Kaggle GPU results alongside your existing RTX 4050 results.\n",
    "# You'll need a GitHub Personal Access Token.\n",
    "\n",
    "# os.chdir(WORKSPACE)\n",
    "# !git config user.email \"your-email@example.com\"\n",
    "# !git config user.name \"Your Name\"\n",
    "# !git add eval/results/\n",
    "# !git commit -m \"Add Kaggle T4 GPU evaluation results\"\n",
    "# !git push origin main\n",
    "# # For private repos: git push https://<TOKEN>@github.com/Sasankamadura/FYP-Final-Testing.git main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d973fcd",
   "metadata": {},
   "source": [
    "---\n",
    "## üí° Tips & Troubleshooting\n",
    "\n",
    "### Resume Support\n",
    "The validation script **automatically skips** models that have already been evaluated.\n",
    "If your Kaggle session dies mid-run, just re-run Step 5 ‚Äî it picks up where it left off.\n",
    "\n",
    "### Kaggle Dataset Setup\n",
    "| Step | Details |\n",
    "|------|---------|\n",
    "| **Upload checkpoints** | kaggle.com ‚Üí Datasets ‚Üí New Dataset ‚Üí upload the two `Checkpoints` folders |\n",
    "| **Upload VisDrone data** | Same process for `VisDrone Val image set` and `VisDrone test image set` |\n",
    "| **Add to notebook** | Click 'Add Data' in the right panel ‚Üí search for your datasets |\n",
    "| **Check paths** | Look at `/kaggle/input/` to see exact folder names |\n",
    "\n",
    "### Cross-GPU Comparison\n",
    "Your repo already has results from `GPU_NVIDIA_GeForce_RTX_4050_Laptop_GPU`.\n",
    "After running on Kaggle (T4), the report generator will automatically produce a\n",
    "**cross-GPU FPS comparison** table.\n",
    "\n",
    "### Performance Tips\n",
    "| Tip | Details |\n",
    "|-----|---------|\n",
    "| **Kaggle GPU quota** | Free tier: 30 hrs/week GPU. Use wisely! |\n",
    "| **T4 x2** | Select T4 x2 accelerator for best compatibility |\n",
    "| **Single model test** | Use `--model baseline_rtdetr_r18` flag to test one model quickly |\n",
    "| **E2E benchmark** | Add `--e2e` flag to also measure full pipeline |\n",
    "| **Session timeout** | Kaggle sessions timeout after ~12 hrs. Save results periodically |\n",
    "\n",
    "### Common Issues\n",
    "| Issue | Fix |\n",
    "|-------|-----|\n",
    "| `CUDAExecutionProvider not available` | Settings ‚Üí Accelerator ‚Üí GPU T4 x2 |\n",
    "| `FileNotFoundError` on `.onnx` file | Check symlinks in Step 3b ‚Äî dataset paths must match |\n",
    "| `Dataset not found` | Click 'Add Data' button ‚Üí search and add your uploaded datasets |\n",
    "| `Session crashed` during validation | Re-run Step 5 (resume support will skip completed models) |\n",
    "| `Read-only filesystem` error | Kaggle input is read-only; results write to `/kaggle/working/` |\n",
    "| `git clone` fails (private repo) | Use `!git clone https://<TOKEN>@github.com/...` |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
